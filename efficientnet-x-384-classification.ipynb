{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install -q efficientnet\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\nimport efficientnet.tfkeras as efn\nimport dill\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\nfrom keras.applications import InceptionResNetV2 as res_inc\nfrom keras.applications import ResNet152 as res_152\nfrom keras.applications import ResNet101 as res_101\nfrom keras.applications import ResNet50 as res_50\nfrom PIL import Image\nfrom os import makedirs","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\r\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":2,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_PATH = KaggleDatasets().get_gcs_path('melanoma-384x384')\n\n# Configuration\nEPOCHS = 40\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nAUG_BATCH = BATCH_SIZE\nIMAGE_SIZE = [384, 384]\n#IMAGE_SIZE = [224,224]\n#IMAGE_SIZE = [128, 128]\n# Seed\nSEED = 123\n# Learning rate\nLR = 0.0003\n# cutmix prob\ncutmix_rate = 0.30\n\n# training filenames directory\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/train*.tfrec')\n# test filenames directory\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '/test*.tfrec')\n# submission file\nSUB = pd.read_csv('/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation / 180.\n    shear = math.pi * shear / 180.\n    \n    # ROTATION MATRIX\n    c1 = tf.math.cos(rotation)\n    s1 = tf.math.sin(rotation)\n    one = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n        \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)\n    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n    \n    # ZOOM MATRIX\n    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n    \n    # SHIFT MATRIX\n    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n\ndef transform(image, label):\n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    DIM = IMAGE_SIZE[0]\n    XDIM = DIM%2 #fix for size 331\n    \n    tmp = random.uniform(0, 1)\n    if 0 < tmp <= 0.1:\n        rot = 15.0 * tf.random.normal([1],dtype='float32')\n    elif 0.1 < tmp <= 0.2:\n        rot = 30.0 * tf.random.normal([1],dtype='float32')\n    elif 0.2 < tmp <= 0.3:\n        rot = 45.0 * tf.random.normal([1],dtype='float32')\n    elif 0.3 < tmp <= 0.4:\n        rot = 60.0 * tf.random.normal([1],dtype='float32')\n    elif 0.4 < tmp <= 0.5:\n        rot = 75.0 * tf.random.normal([1],dtype='float32')\n    elif 0.5 < tmp <= 0.6:\n        rot = 90.0 * tf.random.normal([1],dtype='float32')\n    elif 0.6 < tmp <= 0.7:\n        rot = 110.0 * tf.random.normal([1],dtype='float32')\n    elif 0.7 < tmp <= 0.8:\n        rot = 130.0 * tf.random.normal([1],dtype='float32')\n    elif 0.8 < tmp <= 0.9:\n        rot = 150.0 * tf.random.normal([1],dtype='float32')\n    elif 0.9 < tmp <= 1.0:\n        rot = 180.0 * tf.random.normal([1],dtype='float32')\n    shr = 5. * tf.random.normal([1],dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n    h_shift = 16. * tf.random.normal([1],dtype='float32') \n    w_shift = 16. * tf.random.normal([1],dtype='float32') \n  \n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n    z = tf.ones([DIM*DIM],dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n    idx2 = K.cast(idx2,dtype='int32')\n    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n    d = tf.gather_nd(image['inp1'],tf.transpose(idx3))\n        \n    return {'inp1': tf.reshape(d,[DIM,DIM,3]), 'inp2': image['inp2']}, label\n\n# function to apply cutmix augmentation\ndef cutmix(image, label):\n    # input image - is a batch of images of size [n,dim,dim,3] not a single image of [dim,dim,3]\n    # output - a batch of images with cutmix applied\n    \n    DIM = IMAGE_SIZE[0]    \n    imgs = []; labs = []\n    \n    for j in range(BATCH_SIZE):\n        \n        #random_uniform( shape, minval=0, maxval=None)        \n        # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n        P = tf.cast(tf.random.uniform([], 0, 1) <= cutmix_rate, tf.int32)\n        \n        # CHOOSE RANDOM IMAGE TO CUTMIX WITH\n        k = tf.cast(tf.random.uniform([], 0, BATCH_SIZE), tf.int32)\n        \n        # CHOOSE RANDOM LOCATION\n        x = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n        y = tf.cast(tf.random.uniform([], 0, DIM), tf.int32)\n        \n        # Beta(1, 1)\n        b = tf.random.uniform([], 0, 1) # this is beta dist with alpha=1.0\n        \n\n        WIDTH = tf.cast(DIM * tf.math.sqrt(1-b),tf.int32) * P\n        ya = tf.math.maximum(0,y-WIDTH//2)\n        yb = tf.math.minimum(DIM,y+WIDTH//2)\n        xa = tf.math.maximum(0,x-WIDTH//2)\n        xb = tf.math.minimum(DIM,x+WIDTH//2)\n        \n        # MAKE CUTMIX IMAGE\n        one = image['inp1'][j,ya:yb,0:xa,:]\n        two = image['inp1'][k,ya:yb,xa:xb,:]\n        three = image['inp1'][j,ya:yb,xb:DIM,:]        \n        #ya:yb\n        middle = tf.concat([one,two,three],axis=1)\n\n        img = tf.concat([image['inp1'][j,0:ya,:,:],middle,image['inp1'][j,yb:DIM,:,:]],axis=0)\n        imgs.append(img)\n        \n        # MAKE CUTMIX LABEL\n        a = tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n        lab1 = label[j,]\n        lab2 = label[k,]\n        labs.append((1-a)*lab1 + a*lab2)\n\n    image2 = tf.reshape(tf.stack(imgs),(BATCH_SIZE,DIM,DIM,3))\n    label2 = tf.reshape(tf.stack(labs),(BATCH_SIZE, 1))\n    return {'inp1': image2, 'inp2': image['inp2']}, label2\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\n# function to decode our images (normalize and reshape)\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    # convert image to floats in [0, 1] range\n    image = tf.cast(image, tf.float32) / 255.0 \n    # explicit size needed for TPU\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    return image\n\n# this function parse our images and also get the target variable\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        # tf.string means bytestring\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        # shape [] means single element\n        \"target\": tf.io.FixedLenFeature([], tf.int64),\n        # meta features\n        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n        \n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['target'], tf.float32)\n    # meta features\n    data = {}\n    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n    data['sex'] = tf.cast(example['sex'], tf.int32)\n    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n    # returns a dataset of (image, label, data)\n    return image, label, data\n\n# this function parse our image and also get our image_name (id) to perform predictions\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        # tf.string means bytestring\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        # shape [] means single element\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        # meta features\n        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    image_name = example['image_name']\n    # meta features\n    data = {}\n    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n    data['sex'] = tf.cast(example['sex'], tf.int32)\n    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n    # returns a dataset of (image, key, data)\n    return image, image_name, data\n    \ndef load_dataset(filenames, labeled = True, ordered = False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # Diregarding data order. Order does not matter since we will be shuffling the data anyway\n    \n    ignore_order = tf.data.Options()\n    if not ordered:\n        # disable order, increase speed\n        ignore_order.experimental_deterministic = False \n        \n    # automatically interleaves reads from multiple files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n    # use data as soon as it streams in, rather than in its original order\n    dataset = dataset.with_options(ignore_order)\n    # returns a dataset of (image, label) pairs if labeled = True or (image, id) pair if labeld = False\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO) \n    return dataset\n\n# function for training and validation dataset\ndef setup_input1(image, label, data):\n    \n    # get anatom site general challenge vectors\n    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n    \n    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n    \n    tabular = tf.stack(tab_data + anatom)\n    \n    return {'inp1': image, 'inp2':  tabular}, label\n\n# function for the test set\ndef setup_input2(image, image_name, data):\n    \n    # get anatom site general challenge vectors\n    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n    \n    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n    \n    tabular = tf.stack(tab_data + anatom)\n    \n    return {'inp1': image, 'inp2':  tabular}, image_name\n\n# function for the validation (image name)\ndef setup_input3(image, image_name, target, data):\n    \n    # get anatom site general challenge vectors\n    anatom = [tf.cast(data['anatom_site_general_challenge'][i], dtype = tf.float32) for i in range(7)]\n    \n    tab_data = [tf.cast(data[tfeat], dtype = tf.float32) for tfeat in ['age_approx', 'sex']]\n    \n    tabular = tf.stack(tab_data + anatom)\n    \n    return {'inp1': image, 'inp2':  tabular}, image_name, target\n\ndef data_augment(data, label):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement \n    # in the next function (below), this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    data['inp1'] = tf.image.random_flip_left_right(data['inp1'])\n    data['inp1'] = tf.image.random_flip_up_down(data['inp1'])\n    data['inp1'] = tf.image.random_hue(data['inp1'], 0.01)\n    data['inp1'] = tf.image.random_saturation(data['inp1'], 0.7, 1.3)\n    data['inp1'] = tf.image.random_contrast(data['inp1'], 0.8, 1.2)\n    data['inp1'] = tf.image.random_brightness(data['inp1'], 0.1)\n    \n    return data, label\n\ndef get_training_dataset(filenames, labeled = True, ordered = False):\n    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n    dataset = dataset.map(setup_input1, num_parallel_calls = AUTO)\n    dataset = dataset.map(data_augment, num_parallel_calls = AUTO)\n    dataset = dataset.map(transform, num_parallel_calls = AUTO)\n    # the training dataset must repeat for several epochs\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef get_validation_dataset(filenames, labeled = True, ordered = True):\n    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n    dataset = dataset.map(setup_input1, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    # using gpu, not enought memory to use cache\n    # dataset = dataset.cache()\n    # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.prefetch(AUTO) \n    return dataset\n\ndef get_test_dataset(filenames, labeled = False, ordered = True):\n    dataset = load_dataset(filenames, labeled = labeled, ordered = ordered)\n    dataset = dataset.map(setup_input2, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.prefetch(AUTO) \n    return dataset\n\n# function to count how many photos we have in\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\n# this function parse our images and also get the target variable\ndef read_tfrecord_full(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"image_name\": tf.io.FixedLenFeature([], tf.string), \n        \"target\": tf.io.FixedLenFeature([], tf.int64), \n        # meta features\n        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),\n        \"sex\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_general_challenge\": tf.io.FixedLenFeature([], tf.int64)\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    image_name = example['image_name']\n    target = tf.cast(example['target'], tf.float32)\n    # meta features\n    data = {}\n    data['age_approx'] = tf.cast(example['age_approx'], tf.int32)\n    data['sex'] = tf.cast(example['sex'], tf.int32)\n    data['anatom_site_general_challenge'] = tf.cast(tf.one_hot(example['anatom_site_general_challenge'], 7), tf.int32)\n    return image, image_name, target, data\n\ndef load_dataset_full(filenames):        \n    # automatically interleaves reads from multiple files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO)\n    # returns a dataset of (image_name, target)\n    dataset = dataset.map(read_tfrecord_full, num_parallel_calls = AUTO) \n    return dataset\n\ndef get_data_full(filenames):\n    dataset = load_dataset_full(filenames)\n    dataset = dataset.map(setup_input3, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n\n\nNUM_TRAINING_IMAGES = int(count_data_items(TRAINING_FILENAMES) * 0.8)\n# use validation data for training\nNUM_VALIDATION_IMAGES = int(count_data_items(TRAINING_FILENAMES) * 0.2)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n\nprint('Dataset: {} training images, {} validation images, {} unlabeled test images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES))","execution_count":4,"outputs":[{"output_type":"stream","text":"Dataset: 26153 training images, 6538 validation images, 10982 unlabeled test images\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"makedirs('modldirs')\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def binary_focal_loss(gamma=2., alpha=.25):\n    \"\"\"\n    Binary form of focal loss.\n      FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n      where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n    References:\n        https://arxiv.org/pdf/1708.02002.pdf\n    Usage:\n     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n    \"\"\"\n    def binary_focal_loss_fixed(y_true, y_pred):\n        \"\"\"\n        :param y_true: A tensor of the same shape as `y_pred`\n        :param y_pred:  A tensor resulting from a sigmoid\n        :return: Output tensor.\n        \"\"\"\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n        epsilon = K.epsilon()\n        # clip to prevent NaN's and Inf's\n        pt_1 = K.clip(pt_1, epsilon, 1. - epsilon)\n        pt_0 = K.clip(pt_0, epsilon, 1. - epsilon)\n\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) \\\n               -K.sum((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n\n    return binary_focal_loss_fixed\n\n\ndef get_model1():\n    \n        with strategy.scope():\n            \n            inp1 = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n            inp2 = tf.keras.layers.Input(shape = (9), name = 'inp2')\n            #efnetb3 = efn.EfficientNetB3(weights = 'imagenet', include_top = False)\n            efnetb6 = efn.EfficientNetB6(weights = 'imagenet', include_top = False)\n            x = efnetb6(inp1)\n            #res152 = res(weights='imagenet')\n            #x= res152(inp1)\n            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n            x1 = tf.keras.layers.Dense(50)(inp2)\n            x1 = tf.keras.layers.BatchNormalization()(x1)\n            x1 = tf.keras.layers.Activation('relu')(x1)\n            concat = tf.keras.layers.concatenate([x, x1])\n            concat = tf.keras.layers.Dense(230, activation = 'relu')(concat)\n            concat = tf.keras.layers.BatchNormalization()(concat)\n            concat = tf.keras.layers.Dropout(0.2)(concat)\n            concat = tf.keras.layers.Dense(181, activation = 'relu')(concat)\n            concat = tf.keras.layers.BatchNormalization()(concat)\n            concat = tf.keras.layers.Dropout(0.2)(concat)\n            output = tf.keras.layers.Dense(1, activation = 'sigmoid')(concat)\n\n\n            model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output])\n\n            opt = tf.keras.optimizers.Adam(learning_rate = LR)\n            #opt = tf.keras.optimizers.RMSprop(learning_rate=LR)\n            # opt = tfa.optimizers.SWA(opt)\n\n            model.compile(\n                optimizer = opt,\n                loss = [binary_focal_loss(gamma = 2.0, alpha = 0.80)],\n                metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n            )\n\n            return model\n\ndef get_model2():\n    \n    \n    with strategy.scope():\n        inp1 = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n        inp2 = tf.keras.layers.Input(shape = (9), name = 'inp2')\n        #efnetb3 = efn.EfficientNetB3(weights = 'imagenet', include_top = False)\n        #x = efnetb3(inp1)\n        res152 = res_152(weights='imagenet')\n        x= res152(inp1)\n        #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x1 = tf.keras.layers.Dense(50)(inp2)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n        x1 = tf.keras.layers.Activation('relu')(x1)\n        concat = tf.keras.layers.concatenate([x, x1])\n        concat = tf.keras.layers.Dense(230, activation = 'relu')(concat)\n        concat = tf.keras.layers.BatchNormalization()(concat)\n        concat = tf.keras.layers.Dropout(0.2)(concat)\n        concat = tf.keras.layers.Dense(181, activation = 'relu')(concat)\n        concat = tf.keras.layers.BatchNormalization()(concat)\n        concat = tf.keras.layers.Dropout(0.2)(concat)\n        output = tf.keras.layers.Dense(1, activation = 'sigmoid')(concat)\n\n\n        model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output])\n\n        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n        #opt = tf.keras.optimizers.RMSprop(learning_rate=LR)\n        # opt = tfa.optimizers.SWA(opt)\n\n        model.compile(\n            optimizer = opt,\n            loss = [binary_focal_loss(gamma = 2.0, alpha = 0.80)],\n            metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n        )\n\n        return model\ndef get_model3():\n    \n    \n    with strategy.scope():\n        inp1 = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n        inp2 = tf.keras.layers.Input(shape = (9), name = 'inp2')\n        #efnetb3 = efn.EfficientNetB3(weights = 'imagenet', include_top = False)\n        #x = efnetb3(inp1)\n        res101 = res_101(weights='imagenet')\n        x= res101(inp1)\n        #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x1 = tf.keras.layers.Dense(50)(inp2)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n        x1 = tf.keras.layers.Activation('relu')(x1)\n        concat = tf.keras.layers.concatenate([x, x1])\n        concat = tf.keras.layers.Dense(230, activation = 'relu')(concat)\n        concat = tf.keras.layers.BatchNormalization()(concat)\n        concat = tf.keras.layers.Dropout(0.2)(concat)\n        concat = tf.keras.layers.Dense(181, activation = 'relu')(concat)\n        concat = tf.keras.layers.BatchNormalization()(concat)\n        concat = tf.keras.layers.Dropout(0.2)(concat)\n        output = tf.keras.layers.Dense(1, activation = 'sigmoid')(concat)\n\n\n        model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output])\n\n        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n        #opt = tf.keras.optimizers.RMSprop(learning_rate=LR)\n        # opt = tfa.optimizers.SWA(opt)\n\n        model.compile(\n            optimizer = opt,\n            loss = [binary_focal_loss(gamma = 2.0, alpha = 0.80)],\n            metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n        )\n\n        return model\ndef get_model4():\n    \n    \n    with strategy.scope():\n        inp1 = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n        inp2 = tf.keras.layers.Input(shape = (9), name = 'inp2')\n        #efnetb3 = efn.EfficientNetB3(weights = 'imagenet', include_top = False)\n        #x = efnetb3(inp1)\n        res50 = res_50(weights='imagenet')\n        x= res50(inp1)\n        #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x1 = tf.keras.layers.Dense(50)(inp2)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n        x1 = tf.keras.layers.Activation('relu')(x1)\n        concat = tf.keras.layers.concatenate([x, x1])\n        concat = tf.keras.layers.Dense(230, activation = 'relu')(concat)\n        concat = tf.keras.layers.BatchNormalization()(concat)\n        concat = tf.keras.layers.Dropout(0.2)(concat)\n        concat = tf.keras.layers.Dense(181, activation = 'relu')(concat)\n        concat = tf.keras.layers.BatchNormalization()(concat)\n        concat = tf.keras.layers.Dropout(0.2)(concat)\n        output = tf.keras.layers.Dense(1, activation = 'sigmoid')(concat)\n\n\n        model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output])\n\n        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n        #opt = tf.keras.optimizers.RMSprop(learning_rate=LR)\n        # opt = tfa.optimizers.SWA(opt)\n\n        model.compile(\n            optimizer = opt,\n            loss = [binary_focal_loss(gamma = 2.0, alpha = 0.80)],\n            metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n        )\n\n        return model\ndef get_model5():\n    \n    \n    with strategy.scope():\n        inp1 = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n        inp2 = tf.keras.layers.Input(shape = (9), name = 'inp2')\n        #efnetb3 = efn.EfficientNetB3(weights = 'imagenet', include_top = False)\n        #x = efnetb3(inp1)\n        resinc = res_inc(weights='imagenet')\n        x= resinc(inp1)\n        #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x1 = tf.keras.layers.Dense(50)(inp2)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n        x1 = tf.keras.layers.Activation('relu')(x1)\n        concat = tf.keras.layers.concatenate([x, x1])\n        concat = tf.keras.layers.Dense(230, activation = 'relu')(concat)\n        concat = tf.keras.layers.BatchNormalization()(concat)\n        concat = tf.keras.layers.Dropout(0.2)(concat)\n        concat = tf.keras.layers.Dense(181, activation = 'relu')(concat)\n        concat = tf.keras.layers.BatchNormalization()(concat)\n        concat = tf.keras.layers.Dropout(0.2)(concat)\n        output = tf.keras.layers.Dense(1, activation = 'sigmoid')(concat)\n\n\n        model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output])\n\n        opt = tf.keras.optimizers.Adam(learning_rate = LR)\n        #opt = tf.keras.optimizers.RMSprop(learning_rate=LR)\n        # opt = tfa.optimizers.SWA(opt)\n\n        model.compile(\n            optimizer = opt,\n            loss = [binary_focal_loss(gamma = 2.0, alpha = 0.80)],\n            metrics = [tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()]\n        )\n\n        return model\n   \ndef train_and_predict(SUB, folds = 4):\n    \n    models = []\n    oof_image_name = []\n    oof_target = []\n    oof_prediction = []\n    i=0\n    \n    # seed everything\n    seed_everything(SEED)\n\n    kfold = KFold(folds, shuffle = True, random_state = SEED)\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(TRAINING_FILENAMES)):\n        print('\\n')\n        print('-'*50)\n        print(f'Training fold {fold + 1}')\n        train_dataset = get_training_dataset([TRAINING_FILENAMES[x] for x in trn_ind], labeled = True, ordered = False)\n        val_dataset = get_validation_dataset([TRAINING_FILENAMES[x] for x in val_ind], labeled = True, ordered = True)\n        K.clear_session()\n        model = get_model1()\n        #model.loadweights(\"../input/keras.model\")\n        # using early stopping using val loss\n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_auc', mode = 'max', patience = 5, \n                                                      verbose = 1, min_delta = 0.0001, restore_best_weights = True)\n        # lr scheduler\n        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_auc', factor = 0.4, patience = 2, verbose = 1, min_delta = 0.0001, mode = 'max')\n        '''\n        classifier = KerasClassifier(build_fn = build_classifier)\n        parameters = {'batch_size': [25, 32],\n              'epochs': [100, 500],\n              'optimizer': ['adam', 'rmsprop']}\n        grid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10)\n        grid_search = grid_search.fit(X_train, y_train)\n        best_parameters = grid_search.best_params_\n        best_accuracy = grid_search.best_score_\n        '''\n        history = model.fit(train_dataset, \n                            steps_per_epoch = STEPS_PER_EPOCH,\n                            epochs = EPOCHS,\n                            callbacks = [early_stopping, cb_lr_schedule],\n                            validation_data = val_dataset,\n                            verbose = 2)\n        #models.append(model)\n        #filename = 'modlsdirs/model_' + str(fold+1) + '.h5'\n        #model.save(filename)\n        # want to predict the validation set and save them for stacking\n        number_of_files = count_data_items([TRAINING_FILENAMES[x] for x in val_ind])\n        dataset = get_data_full([TRAINING_FILENAMES[x] for x in val_ind])\n        # get the image name\n        image_name = dataset.map(lambda image, image_name, target: image_name).unbatch()\n        image_name = next(iter(image_name.batch(number_of_files))).numpy().astype('U')\n        # get the real target\n        target = dataset.map(lambda image, image_name, target: target).unbatch()\n        target = next(iter(target.batch(number_of_files))).numpy()\n        # predict the validation set\n        image = dataset.map(lambda image, image_name, target: image)\n        probabilities = model.predict(image)\n        pred_df = pd.DataFrame({'image_name': image_name, 'prediction':list(np.concatenate(probabilities)), 'target':target})\n        pred_df.to_csv('eNetb6_'+ str(fold+1)+'.csv', index = False)\n        oof_image_name.extend(list(image_name))\n        oof_target.extend(list(target))\n        oof_prediction.extend(list(np.concatenate(probabilities)))\n    \n    print('\\n')\n    print('-'*50)\n    # save oof predictions\n    oof_df = pd.DataFrame({'image_name': oof_image_name, 'target': oof_target, 'predictions': oof_prediction})\n    oof_df.to_csv('ResNet152B3_384.csv', index = False)\n        \n    # since we are splitting the dataset and iterating separately on images and ids, order matters.\n    test_ds = get_test_dataset(TEST_FILENAMES, labeled = False, ordered = True)\n    test_images_ds = test_ds.map(lambda image, image_name: image)\n    \n    print('Computing predictions...')\n    probabilities = np.average([np.concatenate(models[i].predict(test_images_ds)) for i in range(folds)], axis = 0)\n    print('Generating submission.csv file...')\n    test_ids_ds = val_dataset.map(lambda image, image_name: image_name).unbatch()\n    # all in one batch\n    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n    pred_df = pd.DataFrame({'image_name': test_ids, 'target': probabilities})\n    SUB.drop('target', inplace = True, axis = 1)\n    SUB = SUB.merge(pred_df, on = 'image_name')\n    SUB.to_csv('sub_EfficientNet_B7_384.csv', index = False)\n    #modelcheckpoint = ModelCheckpoint(\"keras.model\", savebest_only=True, verbose=1)\n    #modelcheckpoint.save('modelsdir/bestmodel')+ '.h5'\n    return oof_target, oof_prediction, probabilities\n    \noof_target, oof_prediction, probabilities = train_and_predict(SUB)","execution_count":null,"outputs":[{"output_type":"stream","text":"\n\n--------------------------------------------------\nTraining fold 1\nDownloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b6_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n165527552/165527152 [==============================] - 5s 0us/step\nEpoch 1/40\n204/204 - 124s - binary_accuracy: 0.6810 - loss: 0.7071 - auc: 0.5468 - val_binary_accuracy: 0.9795 - val_loss: 0.2918 - val_auc: 0.6746 - lr: 3.0000e-04\nEpoch 2/40\n204/204 - 99s - binary_accuracy: 0.8838 - loss: 0.3407 - auc: 0.6213 - val_binary_accuracy: 0.9701 - val_loss: 0.2026 - val_auc: 0.7818 - lr: 3.0000e-04\nEpoch 3/40\n204/204 - 105s - binary_accuracy: 0.9385 - loss: 0.2576 - auc: 0.7091 - val_binary_accuracy: 0.9591 - val_loss: 0.1890 - val_auc: 0.8316 - lr: 3.0000e-04\nEpoch 4/40\n204/204 - 103s - binary_accuracy: 0.9495 - loss: 0.2429 - auc: 0.7331 - val_binary_accuracy: 0.9803 - val_loss: 0.1780 - val_auc: 0.8434 - lr: 3.0000e-04\nEpoch 5/40\n204/204 - 100s - binary_accuracy: 0.9596 - loss: 0.2162 - auc: 0.7728 - val_binary_accuracy: 0.9762 - val_loss: 0.1689 - val_auc: 0.8617 - lr: 3.0000e-04\nEpoch 6/40\n204/204 - 104s - binary_accuracy: 0.9609 - loss: 0.2047 - auc: 0.8086 - val_binary_accuracy: 0.9752 - val_loss: 0.1632 - val_auc: 0.8749 - lr: 3.0000e-04\nEpoch 7/40\n204/204 - 100s - binary_accuracy: 0.9652 - loss: 0.1971 - auc: 0.8025 - val_binary_accuracy: 0.9822 - val_loss: 0.1666 - val_auc: 0.8882 - lr: 3.0000e-04\nEpoch 8/40\n204/204 - 103s - binary_accuracy: 0.9677 - loss: 0.1877 - auc: 0.8376 - val_binary_accuracy: 0.9748 - val_loss: 0.1557 - val_auc: 0.8911 - lr: 3.0000e-04\nEpoch 9/40\n204/204 - 103s - binary_accuracy: 0.9658 - loss: 0.1880 - auc: 0.8418 - val_binary_accuracy: 0.9732 - val_loss: 0.1560 - val_auc: 0.8968 - lr: 3.0000e-04\nEpoch 10/40\n204/204 - 101s - binary_accuracy: 0.9673 - loss: 0.1825 - auc: 0.8502 - val_binary_accuracy: 0.9495 - val_loss: 0.1628 - val_auc: 0.9062 - lr: 3.0000e-04\nEpoch 11/40\n204/204 - 100s - binary_accuracy: 0.9686 - loss: 0.1780 - auc: 0.8576 - val_binary_accuracy: 0.9821 - val_loss: 0.2550 - val_auc: 0.8261 - lr: 3.0000e-04\nEpoch 12/40\n\nEpoch 00012: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n204/204 - 98s - binary_accuracy: 0.9692 - loss: 0.1841 - auc: 0.8421 - val_binary_accuracy: 0.9821 - val_loss: 0.2127 - val_auc: 0.8339 - lr: 3.0000e-04\nEpoch 13/40\n204/204 - 103s - binary_accuracy: 0.9689 - loss: 0.1746 - auc: 0.8620 - val_binary_accuracy: 0.9779 - val_loss: 0.1485 - val_auc: 0.9096 - lr: 1.2000e-04\nEpoch 14/40\n204/204 - 103s - binary_accuracy: 0.9702 - loss: 0.1640 - auc: 0.8865 - val_binary_accuracy: 0.9570 - val_loss: 0.1508 - val_auc: 0.9114 - lr: 1.2000e-04\nEpoch 15/40\n204/204 - 100s - binary_accuracy: 0.9705 - loss: 0.1544 - auc: 0.8954 - val_binary_accuracy: 0.9579 - val_loss: 0.1506 - val_auc: 0.9177 - lr: 1.2000e-04\nEpoch 16/40\n204/204 - 105s - binary_accuracy: 0.9685 - loss: 0.1486 - auc: 0.9079 - val_binary_accuracy: 0.9718 - val_loss: 0.1451 - val_auc: 0.9182 - lr: 1.2000e-04\nEpoch 17/40\n204/204 - 101s - binary_accuracy: 0.9719 - loss: 0.1446 - auc: 0.9123 - val_binary_accuracy: 0.9726 - val_loss: 0.1454 - val_auc: 0.9100 - lr: 1.2000e-04\nEpoch 18/40\n\nEpoch 00018: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n204/204 - 97s - binary_accuracy: 0.9696 - loss: 0.1418 - auc: 0.9169 - val_binary_accuracy: 0.9659 - val_loss: 0.1476 - val_auc: 0.9135 - lr: 1.2000e-04\nEpoch 19/40\n204/204 - 101s - binary_accuracy: 0.9722 - loss: 0.1354 - auc: 0.9210 - val_binary_accuracy: 0.9734 - val_loss: 0.1474 - val_auc: 0.9139 - lr: 4.8000e-05\nEpoch 20/40\n\nEpoch 00020: ReduceLROnPlateau reducing learning rate to 1.920000067912042e-05.\n204/204 - 98s - binary_accuracy: 0.9737 - loss: 0.1286 - auc: 0.9354 - val_binary_accuracy: 0.9627 - val_loss: 0.1524 - val_auc: 0.9166 - lr: 4.8000e-05\nEpoch 21/40\n204/204 - 104s - binary_accuracy: 0.9732 - loss: 0.1253 - auc: 0.9373 - val_binary_accuracy: 0.9734 - val_loss: 0.1471 - val_auc: 0.9191 - lr: 1.9200e-05\nEpoch 22/40\n204/204 - 99s - binary_accuracy: 0.9729 - loss: 0.1211 - auc: 0.9436 - val_binary_accuracy: 0.9708 - val_loss: 0.1487 - val_auc: 0.9190 - lr: 1.9200e-05\nEpoch 23/40\n\nEpoch 00023: ReduceLROnPlateau reducing learning rate to 7.680000271648168e-06.\n204/204 - 99s - binary_accuracy: 0.9749 - loss: 0.1157 - auc: 0.9504 - val_binary_accuracy: 0.9611 - val_loss: 0.1530 - val_auc: 0.9187 - lr: 1.9200e-05\nEpoch 24/40\n204/204 - 99s - binary_accuracy: 0.9746 - loss: 0.1122 - auc: 0.9501 - val_binary_accuracy: 0.9708 - val_loss: 0.1521 - val_auc: 0.9171 - lr: 7.6800e-06\nEpoch 25/40\n\nEpoch 00025: ReduceLROnPlateau reducing learning rate to 3.0720002541784197e-06.\n204/204 - 97s - binary_accuracy: 0.9732 - loss: 0.1147 - auc: 0.9526 - val_binary_accuracy: 0.9667 - val_loss: 0.1539 - val_auc: 0.9162 - lr: 7.6800e-06\nEpoch 26/40\nRestoring model weights from the end of the best epoch.\n204/204 - 106s - binary_accuracy: 0.9741 - loss: 0.1140 - auc: 0.9523 - val_binary_accuracy: 0.9684 - val_loss: 0.1535 - val_auc: 0.9165 - lr: 3.0720e-06\nEpoch 00026: early stopping\n\n\n--------------------------------------------------\nTraining fold 2\nEpoch 1/40\n204/204 - 120s - binary_accuracy: 0.6769 - loss: 0.7466 - auc: 0.5360 - val_binary_accuracy: 0.9808 - val_loss: 0.2373 - val_auc: 0.6619 - lr: 3.0000e-04\nEpoch 2/40\n204/204 - 100s - binary_accuracy: 0.8833 - loss: 0.3454 - auc: 0.6261 - val_binary_accuracy: 0.9800 - val_loss: 0.1972 - val_auc: 0.7938 - lr: 3.0000e-04\nEpoch 3/40\n204/204 - 105s - binary_accuracy: 0.9362 - loss: 0.2763 - auc: 0.6713 - val_binary_accuracy: 0.9578 - val_loss: 0.1981 - val_auc: 0.8401 - lr: 3.0000e-04\nEpoch 4/40\n204/204 - 99s - binary_accuracy: 0.9514 - loss: 0.2449 - auc: 0.7242 - val_binary_accuracy: 0.9733 - val_loss: 0.1816 - val_auc: 0.8280 - lr: 3.0000e-04\nEpoch 5/40\n204/204 - 101s - binary_accuracy: 0.9603 - loss: 0.2167 - auc: 0.7734 - val_binary_accuracy: 0.9404 - val_loss: 0.1792 - val_auc: 0.8894 - lr: 3.0000e-04\nEpoch 6/40\n204/204 - 102s - binary_accuracy: 0.9646 - loss: 0.2019 - auc: 0.8059 - val_binary_accuracy: 0.9767 - val_loss: 0.1663 - val_auc: 0.8754 - lr: 3.0000e-04\nEpoch 7/40\n\nEpoch 00007: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n204/204 - 96s - binary_accuracy: 0.9653 - loss: 0.2033 - auc: 0.8019 - val_binary_accuracy: 0.9463 - val_loss: 0.1822 - val_auc: 0.8710 - lr: 3.0000e-04\nEpoch 8/40\n204/204 - 101s - binary_accuracy: 0.9636 - loss: 0.1894 - auc: 0.8374 - val_binary_accuracy: 0.9728 - val_loss: 0.1595 - val_auc: 0.8844 - lr: 1.2000e-04\nEpoch 9/40\n204/204 - 100s - binary_accuracy: 0.9664 - loss: 0.1790 - auc: 0.8559 - val_binary_accuracy: 0.9747 - val_loss: 0.1544 - val_auc: 0.8925 - lr: 1.2000e-04\nEpoch 10/40\n204/204 - 100s - binary_accuracy: 0.9681 - loss: 0.1627 - auc: 0.8796 - val_binary_accuracy: 0.9766 - val_loss: 0.1636 - val_auc: 0.8844 - lr: 1.2000e-04\nEpoch 11/40\n\nEpoch 00011: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n204/204 - 102s - binary_accuracy: 0.9671 - loss: 0.1736 - auc: 0.8687 - val_binary_accuracy: 0.9771 - val_loss: 0.1589 - val_auc: 0.8900 - lr: 1.2000e-04\nEpoch 12/40\n204/204 - 101s - binary_accuracy: 0.9692 - loss: 0.1613 - auc: 0.8819 - val_binary_accuracy: 0.9723 - val_loss: 0.1542 - val_auc: 0.8979 - lr: 4.8000e-05\nEpoch 13/40\n204/204 - 104s - binary_accuracy: 0.9711 - loss: 0.1549 - auc: 0.8936 - val_binary_accuracy: 0.9769 - val_loss: 0.1544 - val_auc: 0.9011 - lr: 4.8000e-05\nEpoch 14/40\n204/204 - 103s - binary_accuracy: 0.9694 - loss: 0.1510 - auc: 0.9029 - val_binary_accuracy: 0.9713 - val_loss: 0.1532 - val_auc: 0.9017 - lr: 4.8000e-05\nEpoch 15/40\n204/204 - 98s - binary_accuracy: 0.9678 - loss: 0.1496 - auc: 0.9069 - val_binary_accuracy: 0.9722 - val_loss: 0.1571 - val_auc: 0.8957 - lr: 4.8000e-05\nEpoch 16/40\n204/204 - 105s - binary_accuracy: 0.9683 - loss: 0.1501 - auc: 0.9026 - val_binary_accuracy: 0.9762 - val_loss: 0.1508 - val_auc: 0.9046 - lr: 4.8000e-05\nEpoch 17/40\n204/204 - 100s - binary_accuracy: 0.9702 - loss: 0.1445 - auc: 0.9122 - val_binary_accuracy: 0.9726 - val_loss: 0.1511 - val_auc: 0.9060 - lr: 4.8000e-05\nEpoch 18/40\n204/204 - 101s - binary_accuracy: 0.9716 - loss: 0.1391 - auc: 0.9169 - val_binary_accuracy: 0.9771 - val_loss: 0.1529 - val_auc: 0.9042 - lr: 4.8000e-05\n","name":"stdout"},{"output_type":"stream","text":"Epoch 19/40\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 1.920000067912042e-05.\n204/204 - 101s - binary_accuracy: 0.9705 - loss: 0.1419 - auc: 0.9158 - val_binary_accuracy: 0.9739 - val_loss: 0.1564 - val_auc: 0.8953 - lr: 4.8000e-05\nEpoch 20/40\n204/204 - 98s - binary_accuracy: 0.9696 - loss: 0.1406 - auc: 0.9202 - val_binary_accuracy: 0.9730 - val_loss: 0.1526 - val_auc: 0.8998 - lr: 1.9200e-05\nEpoch 21/40\n204/204 - 105s - binary_accuracy: 0.9702 - loss: 0.1320 - auc: 0.9295 - val_binary_accuracy: 0.9698 - val_loss: 0.1484 - val_auc: 0.9064 - lr: 1.9200e-05\nEpoch 22/40\n204/204 - 102s - binary_accuracy: 0.9710 - loss: 0.1319 - auc: 0.9279 - val_binary_accuracy: 0.9704 - val_loss: 0.1479 - val_auc: 0.9098 - lr: 1.9200e-05\nEpoch 23/40\n204/204 - 100s - binary_accuracy: 0.9734 - loss: 0.1285 - auc: 0.9297 - val_binary_accuracy: 0.9732 - val_loss: 0.1518 - val_auc: 0.9063 - lr: 1.9200e-05\nEpoch 24/40\n\nEpoch 00024: ReduceLROnPlateau reducing learning rate to 7.680000271648168e-06.\n204/204 - 100s - binary_accuracy: 0.9719 - loss: 0.1243 - auc: 0.9383 - val_binary_accuracy: 0.9761 - val_loss: 0.1552 - val_auc: 0.9031 - lr: 1.9200e-05\nEpoch 25/40\n204/204 - 98s - binary_accuracy: 0.9740 - loss: 0.1187 - auc: 0.9466 - val_binary_accuracy: 0.9748 - val_loss: 0.1516 - val_auc: 0.9065 - lr: 7.6800e-06\nEpoch 26/40\n\nEpoch 00026: ReduceLROnPlateau reducing learning rate to 3.0720002541784197e-06.\n204/204 - 102s - binary_accuracy: 0.9731 - loss: 0.1204 - auc: 0.9426 - val_binary_accuracy: 0.9716 - val_loss: 0.1523 - val_auc: 0.9078 - lr: 7.6800e-06\nEpoch 27/40\nRestoring model weights from the end of the best epoch.\n204/204 - 103s - binary_accuracy: 0.9729 - loss: 0.1194 - auc: 0.9458 - val_binary_accuracy: 0.9739 - val_loss: 0.1517 - val_auc: 0.9072 - lr: 3.0720e-06\nEpoch 00027: early stopping\n\n\n--------------------------------------------------\nTraining fold 3\nEpoch 1/40\n204/204 - 139s - binary_accuracy: 0.6836 - loss: 0.7441 - auc: 0.5428 - val_binary_accuracy: 0.9823 - val_loss: 0.2321 - val_auc: 0.6486 - lr: 3.0000e-04\nEpoch 2/40\n204/204 - 102s - binary_accuracy: 0.8874 - loss: 0.3630 - auc: 0.5825 - val_binary_accuracy: 0.9781 - val_loss: 0.2043 - val_auc: 0.7630 - lr: 3.0000e-04\nEpoch 3/40\n204/204 - 101s - binary_accuracy: 0.9360 - loss: 0.2787 - auc: 0.6674 - val_binary_accuracy: 0.9818 - val_loss: 0.1866 - val_auc: 0.8149 - lr: 3.0000e-04\nEpoch 4/40\n204/204 - 105s - binary_accuracy: 0.9507 - loss: 0.2425 - auc: 0.7294 - val_binary_accuracy: 0.9793 - val_loss: 0.1928 - val_auc: 0.8276 - lr: 3.0000e-04\nEpoch 5/40\n204/204 - 104s - binary_accuracy: 0.9591 - loss: 0.2206 - auc: 0.7741 - val_binary_accuracy: 0.9562 - val_loss: 0.1873 - val_auc: 0.8747 - lr: 3.0000e-04\nEpoch 6/40\n204/204 - 101s - binary_accuracy: 0.9634 - loss: 0.2041 - auc: 0.7943 - val_binary_accuracy: 0.9771 - val_loss: 0.1574 - val_auc: 0.8863 - lr: 3.0000e-04\nEpoch 7/40\n204/204 - 104s - binary_accuracy: 0.9649 - loss: 0.1935 - auc: 0.8199 - val_binary_accuracy: 0.9796 - val_loss: 0.1695 - val_auc: 0.8892 - lr: 3.0000e-04\nEpoch 8/40\n204/204 - 100s - binary_accuracy: 0.9643 - loss: 0.1966 - auc: 0.8264 - val_binary_accuracy: 0.9478 - val_loss: 0.1755 - val_auc: 0.8895 - lr: 3.0000e-04\nEpoch 9/40\n204/204 - 100s - binary_accuracy: 0.9674 - loss: 0.1861 - auc: 0.8502 - val_binary_accuracy: 0.9794 - val_loss: 0.1561 - val_auc: 0.8875 - lr: 3.0000e-04\nEpoch 10/40\n\nEpoch 00010: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n204/204 - 98s - binary_accuracy: 0.9697 - loss: 0.1789 - auc: 0.8522 - val_binary_accuracy: 0.9781 - val_loss: 0.1768 - val_auc: 0.8819 - lr: 3.0000e-04\nEpoch 11/40\n204/204 - 99s - binary_accuracy: 0.9695 - loss: 0.1672 - auc: 0.8704 - val_binary_accuracy: 0.9807 - val_loss: 0.1708 - val_auc: 0.8841 - lr: 1.2000e-04\nEpoch 12/40\n204/204 - 104s - binary_accuracy: 0.9710 - loss: 0.1588 - auc: 0.8889 - val_binary_accuracy: 0.9818 - val_loss: 0.1761 - val_auc: 0.8917 - lr: 1.2000e-04\nEpoch 13/40\n204/204 - 97s - binary_accuracy: 0.9702 - loss: 0.1585 - auc: 0.8929 - val_binary_accuracy: 0.9795 - val_loss: 0.1629 - val_auc: 0.8904 - lr: 1.2000e-04\nEpoch 14/40\n\nEpoch 00014: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n204/204 - 101s - binary_accuracy: 0.9697 - loss: 0.1567 - auc: 0.8953 - val_binary_accuracy: 0.9728 - val_loss: 0.1661 - val_auc: 0.8892 - lr: 1.2000e-04\nEpoch 15/40\n204/204 - 103s - binary_accuracy: 0.9714 - loss: 0.1428 - auc: 0.9121 - val_binary_accuracy: 0.9775 - val_loss: 0.1612 - val_auc: 0.8986 - lr: 4.8000e-05\nEpoch 16/40\n204/204 - 98s - binary_accuracy: 0.9701 - loss: 0.1365 - auc: 0.9277 - val_binary_accuracy: 0.9787 - val_loss: 0.1843 - val_auc: 0.8918 - lr: 4.8000e-05\nEpoch 17/40\n\nEpoch 00017: ReduceLROnPlateau reducing learning rate to 1.920000067912042e-05.\n204/204 - 102s - binary_accuracy: 0.9723 - loss: 0.1349 - auc: 0.9206 - val_binary_accuracy: 0.9695 - val_loss: 0.1617 - val_auc: 0.8964 - lr: 4.8000e-05\nEpoch 18/40\n204/204 - 97s - binary_accuracy: 0.9714 - loss: 0.1350 - auc: 0.9247 - val_binary_accuracy: 0.9747 - val_loss: 0.1635 - val_auc: 0.8983 - lr: 1.9200e-05\nEpoch 19/40\n204/204 - 105s - binary_accuracy: 0.9711 - loss: 0.1308 - auc: 0.9335 - val_binary_accuracy: 0.9755 - val_loss: 0.1622 - val_auc: 0.9023 - lr: 1.9200e-05\nEpoch 20/40\n204/204 - 97s - binary_accuracy: 0.9727 - loss: 0.1264 - auc: 0.9398 - val_binary_accuracy: 0.9759 - val_loss: 0.1645 - val_auc: 0.9010 - lr: 1.9200e-05\nEpoch 21/40\n\nEpoch 00021: ReduceLROnPlateau reducing learning rate to 7.680000271648168e-06.\n204/204 - 100s - binary_accuracy: 0.9734 - loss: 0.1266 - auc: 0.9361 - val_binary_accuracy: 0.9731 - val_loss: 0.1653 - val_auc: 0.9010 - lr: 1.9200e-05\nEpoch 22/40\n204/204 - 99s - binary_accuracy: 0.9722 - loss: 0.1256 - auc: 0.9356 - val_binary_accuracy: 0.9761 - val_loss: 0.1671 - val_auc: 0.9009 - lr: 7.6800e-06\nEpoch 23/40\n\nEpoch 00023: ReduceLROnPlateau reducing learning rate to 3.0720002541784197e-06.\n204/204 - 99s - binary_accuracy: 0.9715 - loss: 0.1260 - auc: 0.9385 - val_binary_accuracy: 0.9740 - val_loss: 0.1629 - val_auc: 0.9020 - lr: 7.6800e-06\nEpoch 24/40\nRestoring model weights from the end of the best epoch.\n204/204 - 105s - binary_accuracy: 0.9725 - loss: 0.1202 - auc: 0.9463 - val_binary_accuracy: 0.9752 - val_loss: 0.1654 - val_auc: 0.8997 - lr: 3.0720e-06\nEpoch 00024: early stopping\n\n\n--------------------------------------------------\nTraining fold 4\nEpoch 1/40\n204/204 - 129s - binary_accuracy: 0.6811 - loss: 0.7403 - auc: 0.5553 - val_binary_accuracy: 0.9765 - val_loss: 0.2625 - val_auc: 0.6333 - lr: 3.0000e-04\nEpoch 2/40\n204/204 - 100s - binary_accuracy: 0.8850 - loss: 0.3590 - auc: 0.5975 - val_binary_accuracy: 0.9821 - val_loss: 0.2002 - val_auc: 0.7762 - lr: 3.0000e-04\nEpoch 3/40\n204/204 - 100s - binary_accuracy: 0.9408 - loss: 0.2738 - auc: 0.6499 - val_binary_accuracy: 0.9773 - val_loss: 0.1704 - val_auc: 0.8653 - lr: 3.0000e-04\nEpoch 4/40\n204/204 - 98s - binary_accuracy: 0.9516 - loss: 0.2516 - auc: 0.7108 - val_binary_accuracy: 0.9821 - val_loss: 0.1768 - val_auc: 0.8580 - lr: 3.0000e-04\nEpoch 5/40\n\nEpoch 00005: ReduceLROnPlateau reducing learning rate to 0.00012000000569969416.\n204/204 - 97s - binary_accuracy: 0.9617 - loss: 0.2191 - auc: 0.7652 - val_binary_accuracy: 0.9748 - val_loss: 0.1730 - val_auc: 0.8622 - lr: 3.0000e-04\nEpoch 6/40\n204/204 - 100s - binary_accuracy: 0.9634 - loss: 0.2010 - auc: 0.8008 - val_binary_accuracy: 0.9792 - val_loss: 0.1688 - val_auc: 0.8861 - lr: 1.2000e-04\nEpoch 7/40\n204/204 - 101s - binary_accuracy: 0.9647 - loss: 0.1993 - auc: 0.8050 - val_binary_accuracy: 0.9811 - val_loss: 0.1614 - val_auc: 0.8917 - lr: 1.2000e-04\nEpoch 8/40\n204/204 - 97s - binary_accuracy: 0.9683 - loss: 0.1888 - auc: 0.8307 - val_binary_accuracy: 0.9809 - val_loss: 0.1593 - val_auc: 0.8877 - lr: 1.2000e-04\nEpoch 9/40\n\nEpoch 00009: ReduceLROnPlateau reducing learning rate to 4.8000001697801054e-05.\n204/204 - 96s - binary_accuracy: 0.9654 - loss: 0.1925 - auc: 0.8307 - val_binary_accuracy: 0.9802 - val_loss: 0.1596 - val_auc: 0.8914 - lr: 1.2000e-04\nEpoch 10/40\n204/204 - 102s - binary_accuracy: 0.9691 - loss: 0.1712 - auc: 0.8613 - val_binary_accuracy: 0.9698 - val_loss: 0.1528 - val_auc: 0.8983 - lr: 4.8000e-05\n","name":"stdout"},{"output_type":"stream","text":"Epoch 11/40\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate our out of folds roc auc score\nroc_auc = metrics.roc_auc_score(oof_target, oof_prediction)\nprint('Our out of folds roc auc score is: ', roc_auc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.savetxt('probablity',probabilities,delimiter=';')\nprint(probabilities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/submit/sub_EfficientNetB3_384.csv')\ndata.to_csv('data.csv', index= False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"https://drive.google.com/uc?export=download&id=1y7EiqNAWkIASyX6HJbIIOsqCo8N6EjIC\nimport torchvision\ntorchvision.datasets.utils.download_file_from_google_drive(file_id, root, filename=None, md5=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!conda install -y gdown\nimport gdown \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndat=pd.read_csv(\"../input/irisdata2\")\ndat.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}